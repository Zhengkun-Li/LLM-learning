# -*- coding: utf-8 -*-
"""intro_to_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NJcGKcLKZ8Gc5yI-EkM--zEh0-S5uwy0

#**Introduction to RAG**
### *Michaela Buchanan - Mark III Systems*

This notebook references the Intro to LLM workshop notebook which is linked here:

---
Imports for all the code below. Please run this before anything else!
"""

!pip -q install bitsandbytes

from transformers import AutoTokenizer, FalconForCausalLM, BitsAndBytesConfig
import sys
import os
from sentence_transformers.util import semantic_search, dot_score
from sentence_transformers import SentenceTransformer
import pandas as pd
import torch

"""---
### Introduction

In this notebook we will be going through a basic RAG workflow using a dataset of recipes and [Falcon 7B Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct). The goal will be to see the model use specific information from [this recipe dataset](https://huggingface.co/datasets/Shengtao/recipe) in its responses after we build out the RAG pipeline.

---
### Import Model

The first step is to import our LLM and associated tokenizer, in this case Falcon 7B Instruct. Just like in the Intro to LLM we are going to use a sharded version of the model to avoid running out of RAM in our free tier Google Colab environment. This means that the model is loaded in using more smaller files rather than fewer larger files that cause the compute environement to crash. We are also loading in our in 4 bit precision to avoid running out of GPU memory.
"""

model_name = "vilsonrodrigues/falcon-7b-instruct-sharded"

bb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

falcon_model = FalconForCausalLM.from_pretrained(
    model_name,
    quantization_config=bb_config,
    use_cache=False,
    low_cpu_mem_usage=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, clean_up_tokenization_spaces=True)

"""---
### Baseline Inference

Let's set a baseline of what kind of responses our model gives us without using RAG so that we can hopefully see a difference with RAG. First we set up the prompts we would like to use. Feel free to edit or add to these if you are curious about another prompt.
"""

prompt_task = "Question: "
prompt_end = "\nAnswer: "

prompts = [
    "What two ingredients do I need for two-ingredient pizza dough?",
    "What do I need for a quick tartar sauce?",
    "What should I preheat the oven to to make blueberry muffins?"
]

"""Now we feed them into our model to see what responses we get."""

for prompt in prompts:
  prompt = prompt_task + prompt + prompt_end
  inputs = tokenizer(prompt, return_tensors="pt").to('cuda')

  generate_ids = falcon_model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=25, pad_token_id=tokenizer.eos_token_id)

  print("\n")
  print(tokenizer.decode(generate_ids[0]))
  print("\n------------------------------------------------------------")

print("\n\nAll done")

"""---
### Import Recipe Dataset

The results seem reasonable. However we would like our model to use recipe information from a recipe dataset rather than information it remembers from its pretraining. To start that process we first need to import our dataset. We download it from Hugging Face and grab only the first 500 entries. This is only done to save time for this demo because creating the embeddings for all entries in the dataset takes a lot of time and, since this is just a demo, we don't need all of the dataset entries.
"""

df = pd.read_csv("hf://datasets/Shengtao/recipe/recipe.csv")
df = df.head(500)

df

"""Now we are going to combine the `title`, `directions`, and `ingredients` columns into one entry per row. This will be what we embed for our model to reference using RAG."""

text = []
for i, row in df.iterrows():
    text.append(row['title'] + ": " + row['directions'] + " Ingredients list: " + row['ingredients'])

print("Made recipe list")

"""---
### Embedding

Now we are going to setup our embedding model. We will be using the [SentenceTransformers](https://sbert.net/) library for this example but there are lots of other libraries/frameworks out there for doing RAG.

There are many models available through SentenceTransformers. Some are more accurate but larger and slower to run while others are more lightweight but may suffer in performance. We will be using `all-mpnet-base-v2` which offers a nice balance of size and performance. However if you would like to learn more or try out a different model see the [documentation linked here](https://sbert.net/docs/sentence_transformer/pretrained_models.html).
"""

st_model = SentenceTransformer("all-mpnet-base-v2")

"""In the cell below we take each entry in our list of recipes and embed it using the `all-mpnet-base-v2` model. `.encode` creates our embeddings which we append to a list called `embeddings`."""

embeddings = []

for line in text:
    embedding = st_model.encode(line, convert_to_tensor=True)
    embeddings.append(embedding)

print("Made recipe embeddings")

"""---
### Inference Using RAG

Now we are ready to see the results of using our embeddings in our prompts for `Falcon 7B Instruct`. For each prompt in our list of prompts we run through the loop below. We first encode the prompt and then use `semantic_search` from SentenceTransformers to find the most similar embedded recipes. With the `top_k=2` argument we only get the top two results. Then we reference which index these results correspond to and fetch the appropriate elements from our `text` recipe list. These results are included in the prompt and the prompt is passed to our LLM just like before.
"""

for prompt in prompts:
    prompt = prompt_task + prompt + prompt_end

    prompt_embed = st_model.encode(prompt, convert_to_tensor=True)
    hits = semantic_search(prompt_embed, embeddings, top_k=2)

    result_line1 = hits[0][0]['corpus_id']
    result_line2 = hits[0][1]['corpus_id']

    prompt = text[result_line1] + "\n" + text[result_line2] + "\n\n" + prompt

    inputs = tokenizer(prompt, return_tensors="pt")

    generate_ids = falcon_model.generate(input_ids=inputs.input_ids.to('cuda'), attention_mask=inputs.attention_mask.to('cuda'), max_new_tokens=25, pad_token_id=tokenizer.eos_token_id)

    print("\n")
    print(tokenizer.decode(generate_ids[0]))
    print("\n------------------------------------------------------------")

print("\n\nAll done")